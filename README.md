# Custom 4D-STEM Autoencoder

A ResNet-based convolutional autoencoder for dimensionality reduction and analysis of 4D Scanning Transmission Electron Microscopy (4D-STEM) diffraction patterns.

## ğŸ¯ Key Features

- **Image-size agnostic processing** - Works with any input size (32Ã—32 to 512Ã—512+)
- **ResNet-based architecture** with skip connections and adaptive pooling
- **Regularized training** with multiple loss components (MSE + L1 + contrastive + divergence)
- **Sparse embedding layer** with non-negative activations (32D latent space)
- **Comprehensive evaluation** with PSNR, SSIM, and MSE metrics
- **Professional STEM visualization** with virtual bright/dark field imaging

## ğŸš€ Quick Start

### 1. Installation
```bash
pip install -r requirements.txt
```

### 2. Convert Data
```bash
python scripts/convert_dm4.py \
    --input data/Diffraction_SI.dm4 \
    --output data/train_tensor.pt
```

### 3. Train Model
```bash
python scripts/train.py \
    --data data/train_tensor.pt \
    --output_dir outputs \
    --epochs 50 \
    --device cpu
```

### 4. Generate Embeddings
```bash
python scripts/generate_embeddings.py \
    --input data/train_tensor.pt \
    --checkpoint outputs/ae.ckpt \
    --output outputs/embeddings.pt
```

### 5. Visualize Results
```bash
python scripts/visualise_scan_latents.py \
    --raw data/train_tensor.pt \
    --latents outputs/embeddings.pt \
    --scan 42 114 \
    --outfig outputs/results.png
```

## ğŸ“Š Model Performance

- **Parameter count**: 3,914,404 parameters (consistent across all input sizes)
- **Latent dimension**: 32D sparse embeddings
- **Input flexibility**: 32Ã—32 to 512Ã—512+ diffraction patterns
- **Reconstruction quality**: PSNR, SSIM, and MSE tracking

## ğŸ“ Project Structure

```
Custom_4DSTEM_AE/
â”œâ”€â”€ models/           # Neural network architectures
â”œâ”€â”€ scripts/          # Training and utility scripts  
â”œâ”€â”€ data/             # Input data files
â”œâ”€â”€ outputs/          # Generated results and checkpoints
â”œâ”€â”€ tests/            # Architecture and training tests
â”œâ”€â”€ docs/             # Detailed documentation
â””â”€â”€ experiments.ipynb # Interactive analysis notebook
```

## ğŸ“š Documentation

- **[Model Architecture](docs/models/README.md)** - Detailed network architecture and components
- **[Scripts Usage](docs/scripts/README.md)** - Complete guide to all scripts and utilities
- **[Training Guide](docs/training/README.md)** - Training procedures and hyperparameters
- **[Visualization](docs/visualization/README.md)** - STEM analysis and visualization features
- **[API Reference](docs/API.md)** - Complete API documentation

## ğŸ”¬ Architecture Overview

```
Input (any size) â†’ Encoder â†’ 32D Latent â†’ Decoder â†’ Reconstruction (original size)
                      â†“
                 3 ResNet Blocks + Adaptive Pooling
                      â†“
                 Sparse Embeddings (ReLU)
                      â†“
                 3 ResNet Upsampling Blocks
```

**Key Architecture Features:**
- Adaptive pooling handles variable input sizes
- Conditional batch normalization for small feature maps
- Fixed parameter count across all input resolutions
- Skip connections and residual learning

## ğŸ§ª Loss Function

```
L = MSE(y, Å·) + Î»_actÂ·Lâ‚(a) + Î»_simÂ·L_sim + Î»_divÂ·L_div
```

- **MSE**: Reconstruction fidelity
- **Lâ‚**: Sparsity regularization  
- **L_sim**: Contrastive similarity (embedding diversity)
- **L_div**: Activation divergence (prevents mode collapse)

## ğŸ“ˆ Output Files

### Training Outputs:
- `ae.ckpt` - Trained model checkpoint
- `loss_curve.png` - Training progression
- `*_reconstruction_comparison.png` - Before/after comparisons
- `*_stem_visualization.png` - Virtual field analysis
- `tb_logs/` - TensorBoard logs

### Analysis Outputs:
- `embeddings.pt` - Latent space embeddings
- `latent_mosaic.png` - Spatial latent maps

## ğŸ”§ Requirements

- PyTorch 2.2+
- PyTorch Lightning 2.2+
- scikit-image (SSIM/PSNR)
- matplotlib, numpy, h5py, tqdm
- hyperspy (for .dm4 files)

## ğŸ“ Citation

```bibtex
[TBD]
```

---

**For detailed documentation on specific components, see the [docs/](docs/) directory.**